---
title: "R Notebook"
output: word_document
---

# Chapter 2
## Probability and statistics
### Learn
```{r}
library(foreach)  
library(tidyverse)

n_simulations <- 1000  # num times to run simulation
nrows         <- 5000  
sample_p      <- 0.7   
# values should have
dist_mean     <-0    
dist_sd       <- 1
# for p-hacking!
p_threshold   <-0.05

sim_results <- foreach(r=seq_len(n_simulations), .combine = c)  %do% { 
  
  
  #do in series 
  
  #run simulation n_simulations, starts at 1 to x iteration 
# what things do we want to build our sim over.  .combine = comnines stuff depending on the function you specify, in this case the c function, which combines stuff to a vector. 
  
outcome_var   <- rnorm(nrows, mean = dist_mean, sd= dist_sd)  #picks random number from normal distribution and having mean and normal distr we specif.
use_for_training <- sample (1:nrows, nrows*sample_p)  
training <- outcome_var[use_for_training]
testing <- outcome_var[-use_for_training]
sim_ttest <-  t.test(training, testing)

ifelse(sim_ttest$p.value > p_threshold,
       "Means the same!", "Means not the same")

}


sim_results %>% 
  fct_count() %>%   # builds a frequency table,  
  mutate(prop=scales::percent(n/sum(n)))  # n is the number of sum of cuonted value

```


```{r}
seq_len(10)
```


### Changing from a sim with fix parameters to variable parameters
```{r}
library(foreach)  
library(tidyverse)

r_simulations <- seq_len(1)  # num times to run simulation
nrows         <- seq(5000, 200000, by=10000)  
sample_p      <- c(0.7, 0.8, 0.9)   
# values should have
dist_mean     <-0    
dist_sd       <- 1:10
# for p-hacking!
p_threshold   <- c(0.05, 0.1)  # for p-hacking

#how many simulation will we do?
#create a combos table
combos <- expand.grid(r=r_simulations,
                       n = nrows,
                       s=sample_p,
                       m=dist_mean,
                       sd = dist_sd,
                       p=p_threshold)

nrow(combos)


sim_results <- foreach(t= 1:nrow(combos),  #tablerow
                       .combine = rbind)    %do% {   

df <-combos[t,]
outcome_var   <- rnorm(df$n, mean = df$m, sd= df$sd)  
#picks random number from normal distribution and having mean and normal distr we specif.
                         
# rbind returns a singe row dataframe combines to a bid dataframe
#do in series 
#run simulation n_simulations, starts at 1 to x iteration 
# what things do we want to build our sim over.  .combine = comnines stuff depending on the function you specify, in this case the c function, which combines stuff to a vector. 
  
use_for_training <- sample (1:df$n, df$n*df$s)  
training <- outcome_var[use_for_training]
testing <- outcome_var[-use_for_training]
sim_ttest <-  t.test(training, testing)
df$result <- ifelse(sim_ttest$p.value > p, "Same", "Different")

df

#data.frame(
 # r,n,s,m,sd,p,
  #result= ifelse(sim_ttest$p.value > p , 
  #     "Means the same!", "Means not the same")
#)

}

sim_results %>% 
  count(result) %>%   # builds a frequency table,  
  mutate(prop=scales::percent(nn/sum(nn)))  # n is the number of sum of cuonted value



```

### run in parallel cores, 
```{r}
# drawback = managing outputs, slows down, pros = fast running
library(foreach)  
library(tidyverse)
library(parallel)
library(doParallel)

## make running efficient by using all cores when running
# make R aware of that we have a machine with 4 cores.. ish
my_machine <- makeCluster(detectCores())  
#sets up the working environment
registerDoParallel(my_machine)

#how many simulation will we do?
r_simulations <- seq_len(10)  # num times to run simulation
nrows         <- seq(5000, 200000, by=10000)  
sample_p      <- c(0.7, 0.8, 0.9)   
# values should have
dist_mean     <-0    
dist_sd       <- 1:10
# for p-hacking!
p_threshold   <- c(0.05, 0.1)  # for p-hacking

# how likely is it to get two different means?

#create a combos table, a combination of all values
combos <- expand.grid(r=r_simulations,
                       n = nrows,
                       s=sample_p,
                       m=dist_mean,
                       sd = dist_sd,
                       p=p_threshold)

nrow(combos)


sim_results <- foreach(t= 1:nrow(combos),  #tablerow
                       .combine = rbind)    %dopar% {   

df <-combos[t,]
outcome_var   <- rnorm(df$n, mean = df$m, sd= df$sd)  
#picks random number from normal distribution and having mean and normal distr we specif.
                         
# rbind returns a singe row dataframe combines to a bid dataframe
#do in series 
#run simulation n_simulations, starts at 1 to x iteration 
# what things do we want to build our sim over.  .combine = comnines stuff depending on the function you specify, in this case the c function, which combines stuff to a vector. 
  
use_for_training <- sample (1:df$n, df$n*df$s)  # produces a vector of length df$n, and selects the percentage of the sample
training <- outcome_var[use_for_training]
testing <- outcome_var[-use_for_training]
sim_ttest <-  t.test(training, testing)   # comparing mean values 
df$result <- ifelse(sim_ttest$p.value > df$p, "Same", "Different")  # extract the p value from t.test, and comparing with threashold df%p, adds a new column result.

df  # output the data, to get the information about the initial set of data, to analyze it later, passes the result to "foreach" to save it to outcome_var

#data.frame(
 # r,n,s,m,sd,p,
  #result= ifelse(sim_ttest$p.value > p , 
  #     "Means the same!", "Means not the same")
#)

}

sim_results %>% 
  count(result) %>%   # builds a frequency table,  
  mutate(prop=scales::percent(nn/sum(nn)))  # n is the number of sum of cuonted value


library(ggplot2)
sim_results %>% 
  mutate(sim=row_number()) %>% 
  gather(lever, value, -r, -sim, -result)  %>%  # unpivot action, creates lever w
  count(lever, value, result)   %>%  # group by and count * from SQL pers.  
  filter(result!="Same") %>% 
  filter(lever!= "m") %>% 
  ggplot(aes(x=value, y=n)) + 
    geom_line(color="blue") +
    facet_wrap(~lever, scales = "free") + # make a chart per liver parameter
    theme_minimal() +
    geom_smooth()



```




